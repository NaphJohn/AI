{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-keras-GRU-VAEmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUoHCGtL91x/2LeAePXN1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkaikai/AI/blob/main/mnist_keras_GRU_VAEmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.layers import Input, InputLayer, Dense, RepeatVector, Lambda, TimeDistributed\n",
        "# from keras.layers import GRU\n",
        "from keras.layers import CuDNNGRU as GRU  # GPU用\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "# from keras.optimizers import adam\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "YT4Fg4qgKL2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N_TNE7ZdKB52",
        "outputId": "a4aa7f90-a634-4e3c-cd72-8acff16f8dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================preparating the data...====================\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "====================summary of this model====================\n",
            "reconstruction KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_mean/Mean:0', description=\"created by layer 'tf.math.reduce_mean'\") kl KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='tf.math.multiply/Mul:0', description=\"created by layer 'tf.math.multiply'\")\n",
            "seq_vaeの構成\n",
            "Model: \"seq_vae\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
            "                                                                                                  \n",
            " cu_dnngru (CuDNNGRU)           (None, 40)           8400        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 3)            123         ['cu_dnngru[0][0]']              \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 3)            123         ['cu_dnngru[0][0]']              \n",
            "                                                                                                  \n",
            " z (Lambda)                     (None, 3)            0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 28, 3)        0           ['z[0][0]']                      \n",
            "                                                                                                  \n",
            " cu_dnngru_1 (CuDNNGRU)         (None, 28, 40)       5400        ['repeat_vector[0][0]']          \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, 28, 28)      1148        ['cu_dnngru_1[0][0]']            \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            " encoder (Functional)           [(None, 3),          8646        ['input_1[0][0]']                \n",
            "                                 (None, 3),                                                       \n",
            "                                 (None, 3)]                                                       \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 3)           0           ['encoder[0][1]']                \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.square (TFOpLambda)    (None, 3)            0           ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            " tf.reshape (TFOpLambda)        (None,)              0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.reshape_1 (TFOpLambda)      (None,)              0           ['time_distributed[0][0]']       \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 3)            0           ['tf.__operators__.add[0][0]',   \n",
            "                                                                  'tf.math.square[0][0]']         \n",
            "                                                                                                  \n",
            " tf.math.exp (TFOpLambda)       (None, 3)            0           ['encoder[0][1]']                \n",
            "                                                                                                  \n",
            " tf.cast (TFOpLambda)           (None,)              0           ['tf.reshape[0][0]']             \n",
            "                                                                                                  \n",
            " tf.convert_to_tensor (TFOpLamb  (None,)             0           ['tf.reshape_1[0][0]']           \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 3)           0           ['tf.math.subtract[0][0]',       \n",
            " )                                                                'tf.math.exp[0][0]']            \n",
            "                                                                                                  \n",
            " tf.keras.backend.binary_crosse  (None,)             0           ['tf.cast[0][0]',                \n",
            " ntropy (TFOpLambda)                                              'tf.convert_to_tensor[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum (TFOpLambda  (None,)             0           ['tf.math.subtract_1[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  ()                  0           ['tf.keras.backend.binary_crossen\n",
            " a)                                                              tropy[0][0]']                    \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None,)              0           ['tf.math.reduce_sum[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  ()                  0           ['tf.math.reduce_mean[0][0]']    \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None,)             0           ['tf.math.multiply[0][0]']       \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None,)             0           ['tf.math.multiply_1[0][0]',     \n",
            " mbda)                                                            'tf.math.multiply_2[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFOpLam  ()                  0           ['tf.__operators__.add_1[0][0]'] \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " add_loss (AddLoss)             ()                   0           ['tf.math.reduce_mean_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,194\n",
            "Trainable params: 15,194\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "844/844 [==============================] - 18s 11ms/step - loss: 0.3280 - val_loss: 0.2935\n",
            "Epoch 2/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2834 - val_loss: 0.2688\n",
            "Epoch 3/100\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.2653 - val_loss: 0.2617\n",
            "Epoch 4/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2580 - val_loss: 0.2564\n",
            "Epoch 5/100\n",
            "844/844 [==============================] - 12s 14ms/step - loss: 0.2557 - val_loss: 0.2553\n",
            "Epoch 6/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2546 - val_loss: 0.2548\n",
            "Epoch 7/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2537 - val_loss: 0.2536\n",
            "Epoch 8/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2528 - val_loss: 0.2526\n",
            "Epoch 9/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2520 - val_loss: 0.2511\n",
            "Epoch 10/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2505 - val_loss: 0.2487\n",
            "Epoch 11/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2474 - val_loss: 0.2457\n",
            "Epoch 12/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2450 - val_loss: 0.2438\n",
            "Epoch 13/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2435 - val_loss: 0.2419\n",
            "Epoch 14/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2425 - val_loss: 0.2413\n",
            "Epoch 15/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2416 - val_loss: 0.2405\n",
            "Epoch 16/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2407 - val_loss: 0.2389\n",
            "Epoch 17/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2400 - val_loss: 0.2389\n",
            "Epoch 18/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2395 - val_loss: 0.2379\n",
            "Epoch 19/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2390 - val_loss: 0.2374\n",
            "Epoch 20/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2384 - val_loss: 0.2380\n",
            "Epoch 21/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2376 - val_loss: 0.2373\n",
            "Epoch 22/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2364 - val_loss: 0.2350\n",
            "Epoch 23/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2350 - val_loss: 0.2342\n",
            "Epoch 24/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2338 - val_loss: 0.2326\n",
            "Epoch 25/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2328 - val_loss: 0.2317\n",
            "Epoch 26/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2320 - val_loss: 0.2313\n",
            "Epoch 27/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2313 - val_loss: 0.2309\n",
            "Epoch 28/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2309 - val_loss: 0.2297\n",
            "Epoch 29/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2304 - val_loss: 0.2295\n",
            "Epoch 30/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2301 - val_loss: 0.2295\n",
            "Epoch 31/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2296 - val_loss: 0.2286\n",
            "Epoch 32/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2293 - val_loss: 0.2282\n",
            "Epoch 33/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2289 - val_loss: 0.2278\n",
            "Epoch 34/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2288 - val_loss: 0.2278\n",
            "Epoch 35/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2283 - val_loss: 0.2272\n",
            "Epoch 36/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2281 - val_loss: 0.2268\n",
            "Epoch 37/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2279 - val_loss: 0.2269\n",
            "Epoch 38/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2277 - val_loss: 0.2272\n",
            "Epoch 39/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2275 - val_loss: 0.2271\n",
            "Epoch 40/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2273 - val_loss: 0.2267\n",
            "Epoch 41/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2270 - val_loss: 0.2259\n",
            "Epoch 42/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2269 - val_loss: 0.2262\n",
            "Epoch 43/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2268 - val_loss: 0.2255\n",
            "Epoch 44/100\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 0.2266 - val_loss: 0.2265\n",
            "Epoch 45/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2264 - val_loss: 0.2252\n",
            "Epoch 46/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2260 - val_loss: 0.2251\n",
            "Epoch 47/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2259 - val_loss: 0.2248\n",
            "Epoch 48/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2259 - val_loss: 0.2248\n",
            "Epoch 49/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2258 - val_loss: 0.2245\n",
            "Epoch 50/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2256 - val_loss: 0.2248\n",
            "Epoch 51/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2256 - val_loss: 0.2244\n",
            "Epoch 52/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2253 - val_loss: 0.2240\n",
            "Epoch 53/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2254 - val_loss: 0.2241\n",
            "Epoch 54/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2250 - val_loss: 0.2240\n",
            "Epoch 55/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2250 - val_loss: 0.2246\n",
            "Epoch 56/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2248 - val_loss: 0.2236\n",
            "Epoch 57/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2246 - val_loss: 0.2228\n",
            "Epoch 58/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2244 - val_loss: 0.2236\n",
            "Epoch 59/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2244 - val_loss: 0.2234\n",
            "Epoch 60/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2243 - val_loss: 0.2231\n",
            "Epoch 61/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2241 - val_loss: 0.2234\n",
            "Epoch 62/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2240 - val_loss: 0.2229\n",
            "Epoch 63/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2240 - val_loss: 0.2235\n",
            "Epoch 64/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2238 - val_loss: 0.2228\n",
            "Epoch 65/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2238 - val_loss: 0.2232\n",
            "Epoch 66/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2236 - val_loss: 0.2220\n",
            "Epoch 67/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2238 - val_loss: 0.2223\n",
            "Epoch 68/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2240 - val_loss: 0.2225\n",
            "Epoch 69/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2235 - val_loss: 0.2221\n",
            "Epoch 70/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2233 - val_loss: 0.2220\n",
            "Epoch 71/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2232 - val_loss: 0.2219\n",
            "Epoch 72/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2231 - val_loss: 0.2217\n",
            "Epoch 73/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2233 - val_loss: 0.2220\n",
            "Epoch 74/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2232 - val_loss: 0.2218\n",
            "Epoch 75/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2230 - val_loss: 0.2218\n",
            "Epoch 76/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2228 - val_loss: 0.2212\n",
            "Epoch 77/100\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.2228 - val_loss: 0.2213\n",
            "Epoch 78/100\n",
            "844/844 [==============================] - 10s 12ms/step - loss: 0.2228 - val_loss: 0.2219\n",
            "Epoch 79/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2229 - val_loss: 0.2217\n",
            "Epoch 80/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2228 - val_loss: 0.2220\n",
            "Epoch 81/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2225 - val_loss: 0.2217\n",
            "Epoch 82/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2225 - val_loss: 0.2210\n",
            "Epoch 83/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2225 - val_loss: 0.2216\n",
            "Epoch 84/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2227 - val_loss: 0.2216\n",
            "Epoch 85/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2222 - val_loss: 0.2213\n",
            "Epoch 86/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2224 - val_loss: 0.2212\n",
            "Epoch 87/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2223 - val_loss: 0.2212\n",
            "Epoch 88/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2223 - val_loss: 0.2216\n",
            "Epoch 89/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2224 - val_loss: 0.2215\n",
            "Epoch 90/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2222 - val_loss: 0.2208\n",
            "Epoch 91/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2220 - val_loss: 0.2211\n",
            "Epoch 92/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2223 - val_loss: 0.2207\n",
            "Epoch 93/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2219 - val_loss: 0.2205\n",
            "Epoch 94/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2221 - val_loss: 0.2211\n",
            "Epoch 95/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2219 - val_loss: 0.2216\n",
            "Epoch 96/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2218 - val_loss: 0.2204\n",
            "Epoch 97/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2216 - val_loss: 0.2205\n",
            "Epoch 98/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2219 - val_loss: 0.2205\n",
            "Epoch 99/100\n",
            "844/844 [==============================] - 9s 10ms/step - loss: 0.2218 - val_loss: 0.2207\n",
            "Epoch 100/100\n",
            "844/844 [==============================] - 9s 11ms/step - loss: 0.2219 - val_loss: 0.2210\n",
            "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b43aac0eeb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./MNIST_gru_vae/MNIST_gru_vae.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z_mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;31m# 推論\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mclone_graph_nodes\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \"\"\"\n\u001b[0;32m--> 146\u001b[0;31m   \u001b[0mnodes_to_clone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_nodes_by_inputs_and_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0mcloned_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mcloned_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mfind_nodes_by_inputs_and_outputs\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m                          \u001b[0;34m'output tensors. Please make sure the tensor {} is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                          \u001b[0;34m'included in the model inputs when building '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                          'functional model.'.format(kt))\n\u001b[0m\u001b[1;32m    117\u001b[0m       \u001b[0mnodes_to_visit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input tensor cannot be reached given provided output tensors. Please make sure the tensor KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") is included in the model inputs when building functional model."
          ]
        }
      ],
      "source": [
        "def prepare_data():\n",
        "    \"\"\"\n",
        "    use mnist as \n",
        "      x_train, x_test\n",
        "    \"\"\"\n",
        "    from keras.datasets import mnist\n",
        "    (x_train, _), (x_test, _) = mnist.load_data()\n",
        "    x_train, x_test = x_train/255, x_test/255\n",
        "    # >>> x_train.shape, x_test.shape\n",
        "    # ((60000, 28, 28), (10000, 28, 28))\n",
        "    # as (None, NUM_TIMESTEPS, NUM_INPUT_DIM),NUM_TIMESTEPS = 28, NUM_INPUT_DIM = 28\n",
        "    global NUM_TIMESTEPS, NUM_INPUT_DIM\n",
        "    _, NUM_TIMESTEPS, NUM_INPUT_DIM = x_train.shape\n",
        "    return x_train, x_test\n",
        "\n",
        "def seq_vae():\n",
        "    \"\"\"\n",
        "    (input)\n",
        "    ↓\n",
        "    GRU(encoder)\n",
        "    ↓\n",
        "    in GRU\n",
        "    ↓   ↓\n",
        "    mean, log_var\n",
        "    ↓\n",
        "    z--encoder\n",
        "    ↓\n",
        "    GRU(decoder)\n",
        "    ↓\n",
        "    output\n",
        "\n",
        "\n",
        "    train \n",
        "     model\n",
        "    \"\"\"\n",
        "    LATENT_DIM = 40\n",
        "    CODING_DIM=3\n",
        "\n",
        "    def sampling(args):\n",
        "        \"\"\"\n",
        "        从 z_mean, z_log_var = args 中采样 z 的函数\n",
        "         返回值\n",
        "             z (tf.tensor)：采样的潜在变量\n",
        "        \"\"\"\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        dim = K.int_shape(z_mean)[1]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim))\n",
        "        # K.exp (0.5 * z_log_var) 是方差的标准差\n",
        "        # 突然找到标准差是可以的，因为它允许负数。\n",
        "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    # encoder\n",
        "    inputs = Input(shape=(NUM_TIMESTEPS, NUM_INPUT_DIM))\n",
        "    # (None, NUM_TIMESTEPS,NUM_INPUT_DIM)\n",
        "    x = GRU(LATENT_DIM)(inputs)\n",
        "    # (None, CODING_DIM)\n",
        "    z_mean = Dense(CODING_DIM, name='z_mean')(x)  # z_mean输出\n",
        "    # (None, CODING_DIM)\n",
        "    z_log_var = Dense(CODING_DIM, name='z_log_var')(x)  # z_sigma输出\n",
        "    # (None, LATENT_DIM)\n",
        "\n",
        "    # use reparameterization trick to push the sampling out as input\n",
        "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "    z = Lambda(sampling, output_shape=(CODING_DIM,), name='z')(\n",
        "        [z_mean, z_log_var])  # 接收两个变量并随机抽样\n",
        "    # (None, CODING_DIM)\n",
        "    encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    # #encoder 部分接收输入并返回：均值、方差和从中随机采样。\n",
        "\n",
        "    # decoder\n",
        "    latent_inputs = RepeatVector(\n",
        "        NUM_TIMESTEPS)(z)\n",
        "    # (None, NUM_TIMESTEPS, CODING_DIM)\n",
        "    x = GRU(LATENT_DIM, return_sequences=True)(latent_inputs)\n",
        "    # (None, NUM_TIMESTEPS, LATENT_DIM)\n",
        "    outputs = TimeDistributed(\n",
        "        Dense(NUM_INPUT_DIM, activation='sigmoid'))(x)\n",
        "    # (None, NUM_TIMESTEPS, NUM_INPUT_DIM)\n",
        "\n",
        "    # instantiate decoder model\n",
        "    # decoder = Model(z, outputs, name='decoder')\n",
        "    # print(\"decoder构成\")\n",
        "    # decoder.summary()\n",
        "\n",
        "    # 结合解码器和编码器\n",
        "    # 以编码器的输出 z 作为输入运行解码器\n",
        "    vae = Model(inputs, outputs, name='seq_vae')\n",
        "\n",
        "    # 向该模型添加损失函数\n",
        "    def loss(inputs, outputs):\n",
        "        \"\"\"\n",
        "        损失函数定义\n",
        "        \"\"\"\n",
        "        from keras.losses import binary_crossentropy\n",
        "        z_mean, z_log_var, _ = encoder(inputs)\n",
        "        reconstruction_loss = binary_crossentropy(\n",
        "            K.flatten(inputs), K.flatten(outputs))\n",
        "#         reconstruction_loss *= NUM_INPUT_DIM*NUM_TIMESTEPS\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        \n",
        "        lam = 0.01\n",
        "        print(\"reconstruction\",reconstruction_loss,\"kl\",kl_loss)\n",
        "        return K.mean((1-lam)*reconstruction_loss + lam*kl_loss)\n",
        "\n",
        "    vae.add_loss(loss(inputs, outputs))\n",
        "    print(\"seq_vae构成\")\n",
        "    vae.summary()\n",
        "    return vae\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"=\"*20+\"preparating the data...\"+\"=\"*20)\n",
        "\n",
        "    x_train, x_test = prepare_data()\n",
        "    print(\"=\"*20+\"summary of this model\"+\"=\"*20)\n",
        "    seq_vae = seq_vae()\n",
        "    \n",
        "    seq_vae.compile(optimizer=\"amsgrad\")#adam(lr=0.002))\n",
        "    # 学习\n",
        "    seq_vae.fit(x_train,\n",
        "                epochs=100,\n",
        "                batch_size=64,\n",
        "                shuffle=True,\n",
        "                validation_split=0.1,\n",
        "                callbacks=[])\n",
        "                # callbacks=[TensorBoard(log_dir=\"./MNIST_gru_vae/\"), EarlyStopping(patience=3)])\n",
        "\n",
        "    seq_vae.save('./MNIST_gru_vae/MNIST_gru_vae.h5')\n",
        "    encoder=K.function([seq_vae.input],[seq_vae.get_layer(\"z_mean\").output])\n",
        "    decoder=K.function([seq_vae.get_layer(\"z\").output],[seq_vae.layers[-1].output])\n",
        "    # 推断\n",
        "    x_pred = seq_vae.predict(x_test)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    # 预测对应的结果\n",
        "    for true, pred in zip(x_test[::500], x_pred[::500]):\n",
        "        plt.matshow(true)\n",
        "\n",
        "        plt.matshow(pred)\n",
        "        plt.show()\n",
        "\n"
      ]
    }
  ]
}