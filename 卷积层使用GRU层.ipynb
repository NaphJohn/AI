{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "卷积层使用GRU层.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3wKhc2pSC8BZE6v076pxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkaikai/AI/blob/main/%E5%8D%B7%E7%A7%AF%E5%B1%82%E4%BD%BF%E7%94%A8GRU%E5%B1%82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Lambda\n",
        "from keras.layers.core import Dense, Flatten, RepeatVector, Dropout\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.recurrent import GRU\n",
        "# !pip install BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.merge import Concatenate\n",
        "# from .tgru_k2_gpu import TerminalGRU\n",
        "# !pip install TerminalGRU\n",
        "# import TerminalGRU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUT_fgOOMLH1",
        "outputId": "79ec961a-1da4-4254-ad34-b05381a47ddc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement TerminalGRU (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for TerminalGRU\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wHeko2k8C2Wp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================\n",
        "# Encoder functions\n",
        "# =============================\n",
        "def encoder_model(params):\n",
        "    # K_params is dictionary of keras variables\n",
        "    x_in = Input(shape=(params['MAX_LEN'], params[\n",
        "        'NCHARS']), name='input_molecule_smi')\n",
        "\n",
        "    # Convolution layers\n",
        "    x = Convolution1D(int(params['conv_dim_depth'] *\n",
        "                          params['conv_d_growth_factor']),\n",
        "                      int(params['conv_dim_width'] *\n",
        "                          params['conv_w_growth_factor']),\n",
        "                      activation='tanh',\n",
        "                      name=\"encoder_conv0\")(x_in)\n",
        "    if params['batchnorm_conv']:\n",
        "        x = BatchNormalization(axis=-1, name=\"encoder_norm0\")(x)\n",
        "\n",
        "    for j in range(1, params['conv_depth'] - 1):\n",
        "        x = Convolution1D(int(params['conv_dim_depth'] *\n",
        "                              params['conv_d_growth_factor'] ** (j)),\n",
        "                          int(params['conv_dim_width'] *\n",
        "                              params['conv_w_growth_factor'] ** (j)),\n",
        "                          activation='tanh',\n",
        "                          name=\"encoder_conv{}\".format(j))(x)\n",
        "        if params['batchnorm_conv']:\n",
        "            x = BatchNormalization(axis=-1,\n",
        "                                   name=\"encoder_norm{}\".format(j))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Middle layers\n",
        "    if params['middle_layer'] > 0:\n",
        "        middle = Dense(int(params['hidden_dim'] *\n",
        "                           params['hg_growth_factor'] ** (params['middle_layer'] - 1)),\n",
        "                       activation=params['activation'], name='encoder_dense0')(x)\n",
        "        if params['dropout_rate_mid'] > 0:\n",
        "            middle = Dropout(params['dropout_rate_mid'])(middle)\n",
        "        if params['batchnorm_mid']:\n",
        "            middle = BatchNormalization(axis=-1, name='encoder_dense0_norm')(middle)\n",
        "\n",
        "        for i in range(2, params['middle_layer'] + 1):\n",
        "            middle = Dense(int(params['hidden_dim'] *\n",
        "                               params['hg_growth_factor'] ** (params['middle_layer'] - i)),\n",
        "                           activation=params['activation'], name='encoder_dense{}'.format(i))(middle)\n",
        "            if params['dropout_rate_mid'] > 0:\n",
        "                middle = Dropout(params['dropout_rate_mid'])(middle)\n",
        "            if params['batchnorm_mid']:\n",
        "                middle = BatchNormalization(axis=-1,\n",
        "                                            name='encoder_dense{}_norm'.format(i))(middle)\n",
        "    else:\n",
        "        middle = x\n",
        "\n",
        "    z_mean = Dense(params['hidden_dim'], name='z_mean_sample')(middle)\n",
        "\n",
        "    # return both mean and last encoding layer for std dev sampling\n",
        "    return Model(x_in, [z_mean, middle], name=\"encoder\")\n",
        "\n",
        "\n",
        "def load_encoder(params):\n",
        "    # Need to handle K_params somehow... \n",
        "    # Also are we going to be able to save this layer?\n",
        "    # encoder = encoder_model(params, K.constant(0))\n",
        "    # encoder.load_weights(params['encoder_weights_file'])\n",
        "    # return encoder\n",
        "    # !# not sure if this is the right format\n",
        "    return load_model(params['encoder_weights_file'])\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# Decoder functions\n",
        "# ===========================================\n",
        "\n",
        "\n",
        "def decoder_model(params):\n",
        "    z_in = Input(shape=(params['hidden_dim'],), name='decoder_input')\n",
        "    true_seq_in = Input(shape=(params['MAX_LEN'], params['NCHARS']),\n",
        "                        name='decoder_true_seq_input')\n",
        "\n",
        "    z = Dense(int(params['hidden_dim']),\n",
        "              activation=params['activation'],\n",
        "              name=\"decoder_dense0\"\n",
        "              )(z_in)\n",
        "    if params['dropout_rate_mid'] > 0:\n",
        "        z = Dropout(params['dropout_rate_mid'])(z)\n",
        "    if params['batchnorm_mid']:\n",
        "        z = BatchNormalization(axis=-1,\n",
        "                               name=\"decoder_dense0_norm\")(z)\n",
        "\n",
        "    for i in range(1, params['middle_layer']):\n",
        "        z = Dense(int(params['hidden_dim'] *\n",
        "                      params['hg_growth_factor'] ** (i)),\n",
        "                  activation=params['activation'],\n",
        "                  name=\"decoder_dense{}\".format(i))(z)\n",
        "        if params['dropout_rate_mid'] > 0:\n",
        "            z = Dropout(params['dropout_rate_mid'])(z)\n",
        "        if params['batchnorm_mid']:\n",
        "            z = BatchNormalization(axis=-1,\n",
        "                                   name=\"decoder_dense{}_norm\".format(i))(z)\n",
        "\n",
        "    # Necessary for using GRU vectors\n",
        "    z_reps = RepeatVector(params['MAX_LEN'])(z)\n",
        "\n",
        "    # Encoder parts using GRUs\n",
        "    if params['gru_depth'] > 1:\n",
        "        x_dec = GRU(params['recurrent_dim'],\n",
        "                    return_sequences=True, activation='tanh',\n",
        "                    name=\"decoder_gru0\"\n",
        "                    )(z_reps)\n",
        "\n",
        "        for k in range(params['gru_depth'] - 2):\n",
        "            x_dec = GRU(params['recurrent_dim'],\n",
        "                        return_sequences=True, activation='tanh',\n",
        "                        name=\"decoder_gru{}\".format(k + 1)\n",
        "                        )(x_dec)\n",
        "\n",
        "        if params['do_tgru']:\n",
        "            x_out = TerminalGRU(params['NCHARS'],\n",
        "                                rnd_seed=params['RAND_SEED'],\n",
        "                                recurrent_dropout=params['tgru_dropout'],\n",
        "                                return_sequences=True,\n",
        "                                activation='softmax',\n",
        "                                temperature=0.01,\n",
        "                                name='decoder_tgru',\n",
        "                                implementation=params['terminal_GRU_implementation'])([x_dec, true_seq_in])\n",
        "        else:\n",
        "            x_out = GRU(params['NCHARS'],\n",
        "                        return_sequences=True, activation='softmax',\n",
        "                        name='decoder_gru_final')(x_dec)\n",
        "\n",
        "    else:\n",
        "        if params['do_tgru']:\n",
        "            x_out = TerminalGRU(params['NCHARS'],\n",
        "                                rnd_seed=params['RAND_SEED'],\n",
        "                                recurrent_dropout=params['tgru_dropout'],\n",
        "                                return_sequences=True,\n",
        "                                activation='softmax',\n",
        "                                temperature=0.01,\n",
        "                                name='decoder_tgru',\n",
        "                                implementation=params['terminal_GRU_implementation'])([z_reps, true_seq_in])\n",
        "        else:\n",
        "            x_out = GRU(params['NCHARS'],\n",
        "                        return_sequences=True, activation='softmax',\n",
        "                        name='decoder_gru_final'\n",
        "                        )(z_reps)\n",
        "\n",
        "    if params['do_tgru']:\n",
        "        return Model([z_in, true_seq_in], x_out, name=\"decoder\")\n",
        "    else:\n",
        "        return Model(z_in, x_out, name=\"decoder\")\n",
        "\n",
        "\n",
        "def load_decoder(params):\n",
        "    if params['do_tgru']:\n",
        "        return load_model(params['decoder_weights_file'], custom_objects={'TerminalGRU': TerminalGRU})\n",
        "    else:\n",
        "        return load_model(params['decoder_weights_file'])\n",
        "\n",
        "\n",
        "##====================\n",
        "## Middle part (var)\n",
        "##====================\n",
        "\n",
        "def variational_layers(z_mean, enc, kl_loss_var, params):\n",
        "    # @inp mean : mean generated from encoder\n",
        "    # @inp enc : output generated by encoding\n",
        "    # @inp params : parameter dictionary passed throughout entire model.\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "\n",
        "        epsilon = K.random_normal_variable(shape=(params['batch_size'], params['hidden_dim']),\n",
        "                                           mean=0., scale=1.)\n",
        "        # insert kl loss here\n",
        "\n",
        "        z_rand = z_mean + K.exp(z_log_var / 2) * kl_loss_var * epsilon\n",
        "        return K.in_train_phase(z_rand, z_mean)\n",
        "\n",
        "\n",
        "    # variational encoding\n",
        "    z_log_var_layer = Dense(params['hidden_dim'], name='z_log_var_sample')\n",
        "    z_log_var = z_log_var_layer(enc)\n",
        "\n",
        "    z_mean_log_var_output = Concatenate(\n",
        "        name='z_mean_log_var')([z_mean, z_log_var])\n",
        "\n",
        "    z_samp = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "    if params['batchnorm_vae']:\n",
        "        z_samp = BatchNormalization(axis=-1)(z_samp)\n",
        "\n",
        "    return z_samp, z_mean_log_var_output\n",
        "\n",
        "\n",
        "# ====================\n",
        "# Property Prediction\n",
        "# ====================\n",
        "\n",
        "def property_predictor_model(params):\n",
        "    if ('reg_prop_tasks' not in params) and ('logit_prop_tasks' not in params):\n",
        "        raise ValueError('You must specify either regression tasks and/or logistic tasks for property prediction')\n",
        "\n",
        "    ls_in = Input(shape=(params['hidden_dim'],), name='prop_pred_input')\n",
        "\n",
        "    prop_mid = Dense(params['prop_hidden_dim'],\n",
        "                     activation=params['prop_pred_activation'])(ls_in)\n",
        "    if params['prop_pred_dropout'] > 0:\n",
        "        prop_mid = Dropout(params['prop_pred_dropout'])(prop_mid)\n",
        "\n",
        "    if params['prop_pred_depth'] > 1:\n",
        "        for p_i in range(1, params['prop_pred_depth']):\n",
        "            prop_mid = Dense(int(params['prop_hidden_dim'] *\n",
        "                                 params['prop_growth_factor'] ** p_i),\n",
        "                             activation=params['prop_pred_activation'],\n",
        "                             name=\"property_predictor_dense{}\".format(p_i)\n",
        "                             )(prop_mid)\n",
        "            if params['prop_pred_dropout'] > 0:\n",
        "                prop_mid = Dropout(params['prop_pred_dropout'])(prop_mid)\n",
        "            if 'prop_batchnorm' in params and params['prop_batchnorm']:\n",
        "                prop_mid = BatchNormalization()(prop_mid)\n",
        "\n",
        "    # for regression tasks\n",
        "    if ('reg_prop_tasks' in params) and (len(params['reg_prop_tasks']) > 0):\n",
        "        reg_prop_pred = Dense(len(params['reg_prop_tasks']), activation='linear',\n",
        "                              name='reg_property_output')(prop_mid)\n",
        "\n",
        "    # for logistic tasks \n",
        "    if ('logit_prop_tasks' in params) and (len(params['logit_prop_tasks']) > 0):\n",
        "        logit_prop_pred = Dense(len(params['logit_prop_tasks']), activation='sigmoid',\n",
        "                                name='logit_property_output')(prop_mid)\n",
        "\n",
        "    # both regression and logistic\n",
        "    if (('reg_prop_tasks' in params) and (len(params['reg_prop_tasks']) > 0) and\n",
        "            ('logit_prop_tasks' in params) and (len(params['logit_prop_tasks']) > 0)):\n",
        "\n",
        "        return Model(ls_in, [reg_prop_pred, logit_prop_pred], name=\"property_predictor\")\n",
        "\n",
        "        # regression only scenario\n",
        "    elif ('reg_prop_tasks' in params) and (len(params['reg_prop_tasks']) > 0):\n",
        "        return Model(ls_in, reg_prop_pred, name=\"property_predictor\")\n",
        "\n",
        "        # logit only scenario\n",
        "    else:\n",
        "        return Model(ls_in, logit_prop_pred, name=\"property_predictor\")\n",
        "\n",
        "\n",
        "def load_property_predictor(params):\n",
        "    return load_model(params['prop_pred_weights_file'])"
      ]
    }
  ]
}