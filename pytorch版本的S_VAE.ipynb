{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch版本的S-VAE.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8gbw8u0yTrCJbDJlacovv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whkaikai/AI/blob/main/pytorch%E7%89%88%E6%9C%AC%E7%9A%84S_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xIUu_qeakrC",
        "outputId": "24e8147d-50e7-4d78-aa06-17b8dcdb81c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partialsmiles in /usr/local/lib/python3.7/dist-packages (1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ops in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ops) (3.13)\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "!pip install kora -q\n",
        "!pip install partialsmiles\n",
        "\n",
        "import kora.install.rdkit\n",
        "\n",
        "import math\n",
        "import partialsmiles as ps\n",
        "# 验证 SMILES 解析器，支持部分 SMILES（例如，生成模型逐个字符生成）\n",
        "\n",
        "from rdkit import Chem\n",
        "!pip install ops\n",
        "import ops\n",
        "\n",
        "# from utilities import ops\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "fPl068ROyJ9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rsisU3-nyM2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n"
      ],
      "metadata": {
        "id": "5cmdbNXmarHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=256, n_layers=1,\n",
        "                 bidirectional=False, latent_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_factor = (2 if bidirectional else 1) * n_layers\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=input_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=n_layers,\n",
        "                          bidirectional=bidirectional,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.mean_lin = nn.Linear(hidden_size * self.hidden_factor, latent_size)\n",
        "        self.logvar_lin = nn.Linear(hidden_size * self.hidden_factor, latent_size)\n",
        "\n",
        "        ops.init_params(self)\n",
        "\n",
        "\n",
        "    def forward(self, input_embedding):\n",
        "        \"\"\"\n",
        "        :param input_embedding: [batch_size, seq_len, embed_size] tensor\n",
        "        :return: latent vector mean and log var [batchsize, latentsize] \n",
        "        \"\"\"\n",
        "        _, hidden = self.rnn(input_embedding)\n",
        "        hidden = hidden.permute(1, 0, 2)\n",
        "        hidden = hidden.contiguous().view(hidden.size(0), -1)\n",
        "\n",
        "        # reparameterization\n",
        "        mean = self.mean_lin(hidden)\n",
        "        logv = -torch.abs(self.logvar_lin(hidden))\n",
        "        \n",
        "        return mean, logv"
      ],
      "metadata": {
        "id": "hRuR7td4atMT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "gfW1a3KdbHRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size=64, hidden_size=256, n_layers=1,\n",
        "                 dropout=0.5, latent_size=128,\n",
        "                 vocab_size=64, max_len=75, vocab=None, sos_idx=2, padding_idx=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.latent_size = latent_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.vocab = vocab\n",
        "        self.sos_idx = sos_idx\n",
        "        self.padding_idx = padding_idx\n",
        "        self.hidden_factor = n_layers\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(p=dropout)\n",
        "        self.rnn = nn.GRU(input_size=input_size,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=n_layers,\n",
        "                          batch_first=True)\n",
        "\n",
        "        self.latent2hidden = torch.nn.Linear(latent_size, hidden_size * self.hidden_factor)\n",
        "        self.outputs2vocab = torch.nn.Linear(hidden_size, vocab_size)\n",
        "        self.outputs_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        ops.init_params(self)\n",
        "\n",
        "    def forward(self, input_embedding, latent):\n",
        "        hidden = self.latent2hidden(latent)\n",
        "        \n",
        "        hidden = hidden.view(-1, self.hidden_factor, self.hidden_size)\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        input_embedding = self.embedding_dropout(input_embedding)\n",
        "        outputs, _ = self.rnn(input_embedding, hidden)\n",
        "\n",
        "        # process outputs\n",
        "        b, seq_len, hsize = outputs.size()\n",
        "        outputs = outputs.contiguous().view(-1, hsize)\n",
        "        \n",
        "        outputs = self.outputs_dropout(outputs)\n",
        "        outputs = self.outputs2vocab(outputs)\n",
        "        \n",
        "        return outputs.view(b, seq_len, self.vocab_size)\n",
        "\n",
        "    def inference_guided(self, latent, embedding, max_len=None):\n",
        "        if max_len is None:\n",
        "            max_len = self.max_len\n",
        "        assert latent.size(1) == self.latent_size, 'latent size error!'\n",
        "        batch_size = latent.size(0)\n",
        "        hidden = self.latent2hidden(latent)\n",
        "\n",
        "        hidden = hidden.view(batch_size, self.hidden_factor, self.hidden_size)\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        input_sequence = torch.Tensor(batch_size).fill_(self.sos_idx).unsqueeze(1).long()\n",
        "        index_pred = torch.LongTensor()\n",
        "        smiles_pred = [\"\" for _ in range(batch_size)]\n",
        "        smiles_state = [0 for _ in range(batch_size)]  # -1: failed, 1; succeed, 0: todo\n",
        "        for t in range(max_len):\n",
        "            input_sequence = input_sequence.cuda()\n",
        "            input_embedding = embedding(input_sequence)\n",
        "\n",
        "            # decoder rnn run once\n",
        "            output, hidden = self.rnn(input_embedding, hidden)\n",
        "            output = self.outputs_dropout(output)\n",
        "            logits = self.outputs2vocab(output).cpu()\n",
        "            \n",
        "            # prepare next input\n",
        "            input_sequence = torch.argmax(logits, dim=-1)\n",
        "            _, index = torch.sort(logits, dim=-1, descending=True)\n",
        "            for i in range(batch_size):\n",
        "                # check if the smiles finished\n",
        "                if smiles_state[i] != 0:\n",
        "                    continue\n",
        "                # check for all tokens in descending possibility\n",
        "                flag = False\n",
        "                for j in range(logits.size(-1)):\n",
        "                    idx_cur = index[i][0][j]\n",
        "                    if idx_cur == self.padding_idx:\n",
        "                        if Chem.MolFromSmiles(smiles_pred[i]) is not None:\n",
        "                            input_sequence[i] = idx_cur\n",
        "                            smiles_state[i] = 1\n",
        "                            break\n",
        "                    elif idx_cur > 2:\n",
        "                        smi = smiles_pred[i] + self.vocab.itos[idx_cur]\n",
        "                        try:\n",
        "                            ps.ParseSmiles(smi, partial=True)\n",
        "                            input_sequence[i] = idx_cur\n",
        "                            smiles_pred[i] = smi\n",
        "                            flag = True\n",
        "                            break\n",
        "                        except ps.Error as e:\n",
        "                            continue\n",
        "                # failed for all tokens\n",
        "                if not flag:\n",
        "                    smiles_state[i] = -1\n",
        "            index_pred = torch.cat((index_pred, input_sequence), dim=1)\n",
        "            if 0 not in smiles_state:\n",
        "                break\n",
        "        return smiles_pred, index_pred\n",
        "\n",
        "    def inference_direct(self, latent, embedding, max_len=None):\n",
        "        if max_len is None:\n",
        "            max_len = self.max_len\n",
        "        assert latent.size(1) == self.latent_size, 'latent size error!'\n",
        "        batch_size = latent.size(0)\n",
        "        hidden = self.latent2hidden(latent)\n",
        "\n",
        "        hidden = hidden.view(batch_size, self.hidden_factor, self.hidden_size)\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()\n",
        "        hidden = torch.tanh(hidden)\n",
        "\n",
        "        input_sequence = torch.Tensor(batch_size).fill_(self.padding_idx).unsqueeze(1).long()\n",
        "        logits_t = torch.FloatTensor()\n",
        "        for t in range(max_len):\n",
        "            input_sequence = input_sequence.cuda()\n",
        "            input_embedding = embedding(input_sequence)\n",
        "            # decoder rnn run once\n",
        "            output, hidden = self.rnn(input_embedding, hidden)\n",
        "            output = self.outputs_dropout(output)\n",
        "            logits = self.outputs2vocab(output).cpu()\n",
        "            logits_t = torch.cat((logits_t, logits), dim=1)\n",
        "            # prepare next input\n",
        "            input_sequence = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        index_pred = torch.argmax(logits_t, dim=-1)\n",
        "        smiles_pred = []\n",
        "        for i in range(batch_size):\n",
        "            smi = [self.vocab.itos[p] for p in index_pred[i]]\n",
        "            smi = ''.join(smi).split()[0]\n",
        "            smiles_pred.append(smi)\n",
        "        return smiles_pred, index_pred\n",
        "\n",
        "    def inference(self, latent, embedding, max_len=None, partialsmiles=False):\n",
        "        if partialsmiles:\n",
        "            return self.inference_guided(latent, embedding, max_len)\n",
        "        else:\n",
        "            return self.inference_direct(latent, embedding, max_len)"
      ],
      "metadata": {
        "id": "GqjykS8ZbJor"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S-VAE模型编写\n"
      ],
      "metadata": {
        "id": "8rNCDAMtRMz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vae(nn.Module):\n",
        "    def __init__(self, vocab, vocab_size, embedding_size, dropout, padding_idx,\n",
        "            sos_idx, unk_idx, max_len, n_layers, hidden_size,\n",
        "            bidirectional=False, latent_size=128, partialsmiles=False):\n",
        "        super(Vae, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_len = max_len\n",
        "        self.padding_idx = padding_idx  # eos padding \n",
        "        self.sos_idx = sos_idx\n",
        "        self.unk_idx = unk_idx\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.latent_size = latent_size\n",
        "        self.dropout = dropout\n",
        "        self.partialsmiles = partialsmiles\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                      embedding_dim=embedding_size)\n",
        "        \n",
        "        self.encoder = Encoder(input_size=embedding_size,\n",
        "                               hidden_size=hidden_size,\n",
        "                               n_layers=n_layers,\n",
        "                               bidirectional=bidirectional,\n",
        "                               latent_size=latent_size)\n",
        "\n",
        "        dec_layers = (2 if bidirectional else 1) * n_layers\n",
        "        self.decoder = Decoder(input_size=embedding_size,\n",
        "                               hidden_size=hidden_size,\n",
        "                               n_layers=dec_layers,\n",
        "                               dropout=dropout,\n",
        "                               latent_size=latent_size,\n",
        "                               vocab_size=vocab_size,\n",
        "                               max_len=max_len,\n",
        "                               vocab=vocab,\n",
        "                               sos_idx=sos_idx,\n",
        "                               padding_idx=padding_idx)\n",
        "\n",
        "        self.encoder_params = list(self.embedding.parameters()) + list(self.encoder.parameters())\n",
        "        self.decoder_params = self.decoder.parameters()\n",
        "\n",
        "    def forward(self, input_sequence, epsilon_std=1.0):\n",
        "        # input sequence are like: <sos> SMILES tokens <padding>\n",
        "        # for input, remove the last token\n",
        "        input_sequence = input_sequence[:, :-1]\n",
        "        input_embedding, mean, logv, z = self.encoder_sample(input_sequence, epsilon_std)\n",
        "\n",
        "        # decoder output projection on vacab\n",
        "        outputs = self.decoder(input_embedding, z)\n",
        "        return outputs, mean, logv, z\n",
        "\n",
        "    def encoder_sample(self, input_sequence, epsilon_std=1.0):\n",
        "        batch_size = input_sequence.size(0)\n",
        "        # move data onto GPU\n",
        "        input_sequence = input_sequence.cuda()\n",
        "        \n",
        "        # embedding input sequences\n",
        "        input_embedding = self.embedding(input_sequence)\n",
        "        \n",
        "        # encoder process\n",
        "        mean, logv = self.encoder(input_embedding)\n",
        "        \n",
        "        # sample prior\n",
        "        z = ops.sample(mean, logv, (batch_size, self.latent_size), epsilon_std).cuda()\n",
        "        return input_embedding, mean, logv, z\n",
        "\n",
        "    def inference(self, latent, max_len=None):\n",
        "        # decoder inference\n",
        "        outputs = self.decoder.inference(latent, self.embedding, max_len=max_len, partialsmiles=self.partialsmiles)\n",
        "        return outputs\n",
        "\n",
        "    def calc_mi(self, x):\n",
        "        \"\"\"Approximate the mutual information between x and z\n",
        "        I(x, z) = E_xE_{q(z|x)}log(q(z|x)) - E_xE_{q(z|x)}log(q(z))\n",
        "        Returns: Float\n",
        "        \"\"\"\n",
        "        # [x_batch, nz]\n",
        "        _, mu, logvar, z_samples = self.encoder_sample(x)\n",
        "        x_batch, nz = mu.size()\n",
        "        # E_{q(z|x)}log(q(z|x)) = -0.5*nz*log(2*\\pi) - 0.5*(1+logvar).sum(-1)\n",
        "        neg_entropy = (-0.5 * nz * math.log(2 * math.pi)- 0.5 * (1 + logvar).sum(-1)).mean()\n",
        "        var = logvar.exp()\n",
        "        # (z_batch, 1, nz)\n",
        "        z_samples = z_samples.unsqueeze(1)\n",
        "        # (1, x_batch, nz)\n",
        "        mu = mu.unsqueeze(0)\n",
        "        logvar = logvar.unsqueeze(0)\n",
        "        # (z_batch, x_batch, nz)\n",
        "        dev = z_samples - mu\n",
        "        # (z_batch, x_batch)\n",
        "        log_density = -0.5 * ((dev ** 2) / var).sum(dim=-1) - \\\n",
        "            0.5 * (nz * math.log(2 * math.pi) + logvar.sum(-1))\n",
        "        # log q(z): aggregate posterior\n",
        "        log_qz = ops.log_sum_exp(log_density, dim=1) - math.log(x_batch)\n",
        "        return (neg_entropy - log_qz.mean(-1)).item()"
      ],
      "metadata": {
        "id": "ooT4hKRWbY4h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 潜向量层加入之后\n",
        "# 需要修改代码\n",
        "hidden_size = 256\n",
        "hidden_layer_depth = 2\n",
        "latent_length = 10\n",
        "dropout = 0.5\n",
        "bidirectional = True\n",
        "# encoder\n",
        "encoder = Encoder( n_features, hidden_size, hidden_layer_depth, latent_length,bidirectional, dropout)\n",
        "\n",
        "#latent\n",
        "latent = Latent( hidden_size, latent_length, bidirectional)\n",
        "\n",
        "# decoder\n",
        "decoder = Decoder(seq_len, batch_size, hidden_size, hidden_layer_depth, latent_length, n_features,bidirectional)\n",
        "\n",
        "# vae\n",
        "model = VAE(encoder,latent,decoder).to(device)\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "KWyJLjg9-Jqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S-VAE train"
      ],
      "metadata": {
        "id": "G-C3ONiEacBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S-VAE loss"
      ],
      "metadata": {
        "id": "5Rjqbamyatvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(X, x_hat, z_mu, z_var, criterion):\n",
        "  \n",
        "\n",
        "  if criterion == 'MSELoss':\n",
        "    criterion = nn.MSELoss(size_average=False,reduction='sum')\n",
        "  elif criterion == 'SmoothL1Loss':\n",
        "    criterion = nn.SmoothL1Loss(size_average=False,reduction='sum')\n",
        "\n",
        "  recon_loss = criterion(x_hat, X)\n",
        "  KL_loss = -0.5 * torch.mean(1 + z_var - z_mu**2 - torch.exp(z_var))\n",
        "  KL_loss = KL_loss/X.view(-1, X.size(1)).data.shape[0] * X.size(1)\n",
        "  \n",
        "  elbo_loss = recon_loss + KL_loss\n",
        "  \n",
        "  return elbo_loss, KL_loss"
      ],
      "metadata": {
        "id": "T8jg3WWA-g6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.02) # lr derived from optimazation method"
      ],
      "metadata": {
        "id": "uYztImef-nPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU\n"
      ],
      "metadata": {
        "id": "NpbzcoCtahja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUCell(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    An implementation of GRUCell.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
        "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        x = x.view(-1, x.size(1))\n",
        "        \n",
        "        gate_x = self.x2h(x) \n",
        "        gate_h = self.h2h(hidden)\n",
        "        \n",
        "        gate_x = gate_x.squeeze()\n",
        "        gate_h = gate_h.squeeze()\n",
        "        \n",
        "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
        "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
        "        \n",
        "        \n",
        "        resetgate = F.sigmoid(i_r + h_r)\n",
        "        inputgate = F.sigmoid(i_i + h_i)\n",
        "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
        "        \n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        \n",
        "        \n",
        "        return hy"
      ],
      "metadata": {
        "id": "DGhyfhmZCfco"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
        "        super(GRUModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "         \n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "         \n",
        "       \n",
        "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
        "        \n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "     \n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Initialize hidden state with zeros/Use GPU\n",
        "\n",
        "        #print(x.shape,\"x.shape\")100, 28, 28\n",
        "        if torch.cuda.is_available():\n",
        "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
        "        else:\n",
        "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
        "         \n",
        "       \n",
        "        outs = []\n",
        "        \n",
        "        hn = h0[0,:,:]\n",
        "        \n",
        "        for seq in range(x.size(1)):\n",
        "            hn = self.gru_cell(x[:,seq,:], hn) \n",
        "            outs.append(hn)\n",
        "            \n",
        "\n",
        "        out = outs[-1].squeeze()\n",
        "        \n",
        "        out = self.fc(out) \n",
        "        # out.size() --> 100, 10\n",
        "        return out\n",
        " "
      ],
      "metadata": {
        "id": "anJp6O2LCv_F"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}